{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQWX8p27kkf1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSgMsPG5kkf2"
      },
      "outputs": [],
      "source": [
        "# The data shared for NMA projects is a subset of the full HCP dataset\n",
        "N_SUBJECTS = 100\n",
        "\n",
        "# The data have already been aggregated into ROIs from the Glasser parcellation\n",
        "N_PARCELS = 360\n",
        "\n",
        "# The acquisition parameters for all tasks were identical\n",
        "TR = 0.72  # Time resolution, in seconds\n",
        "\n",
        "# The parcels are matched across hemispheres with the same order\n",
        "HEMIS = [\"Right\", \"Left\"]\n",
        "\n",
        "# Each experiment was repeated twice in each subject\n",
        "RUNS   = ['LR','RL']\n",
        "N_RUNS = 2\n",
        "\n",
        "# There are 7 tasks. Each has a number of 'conditions'\n",
        "# TIP: look inside the data folders for more fine-graned conditions\n",
        "\n",
        "EXPERIMENTS = {\n",
        "    'MOTOR'      : {'cond':['lf','rf','lh','rh','t','cue']},\n",
        "    'WM'         : {'cond':['0bk_body','0bk_faces','0bk_places','0bk_tools','2bk_body','2bk_faces','2bk_places','2bk_tools']},\n",
        "    'EMOTION'    : {'cond':['fear','neut']},\n",
        "    'GAMBLING'   : {'cond':['loss','win']},\n",
        "    'LANGUAGE'   : {'cond':['math','story']},\n",
        "    'RELATIONAL' : {'cond':['match','relation']},\n",
        "    'SOCIAL'     : {'cond':['ment','rnd']}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXFsMEyvkkf2"
      },
      "source": [
        "> For a detailed description of the tasks have a look pages 45-54 of the [HCP reference manual](https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFMcTjWUkkf2"
      },
      "outputs": [],
      "source": [
        "# @title Download data file\n",
        "import os, requests\n",
        "\n",
        "fname = \"hcp_task.tgz\"\n",
        "url = \"https://osf.io/2y3fw/download\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVgO5rNlkkf3"
      },
      "outputs": [],
      "source": [
        "# The download cells will store the data in nested directories starting here:\n",
        "HCP_DIR = \"./hcp_task\"\n",
        "\n",
        "# importing the \"tarfile\" module\n",
        "import tarfile\n",
        "\n",
        "# open file\n",
        "with tarfile.open(fname) as tfile:\n",
        "  # extracting file\n",
        "  tfile.extractall('.')\n",
        "\n",
        "subjects = np.loadtxt(os.path.join(HCP_DIR, 'subjects_list.txt'), dtype='str')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z57WE2ykkf3"
      },
      "outputs": [],
      "source": [
        "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
        "region_info = dict(\n",
        "    name=regions[0].tolist(),\n",
        "    network=regions[1],\n",
        "    hemi=['Right']*int(N_PARCELS/2) + ['Left']*int(N_PARCELS/2),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58YSr2UKHuIN",
        "outputId": "432c88b5-5be8-482e-929b-47be8bea77f2"
      },
      "outputs": [],
      "source": [
        "all_ROI_names = region_info['name'][1:]\n",
        "err_txt = \"I trained without the 1st column\"\n",
        "assert len(all_ROI_names) == 359, err_txt\n",
        "print(all_ROI_names)\n",
        "#print(region_info['network'])\n",
        "#print(region_info['hemi'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vciTbPA2kkf3"
      },
      "outputs": [],
      "source": [
        "def load_single_timeseries(subject, experiment, run, remove_mean=True):\n",
        "  \"\"\"Load timeseries data for a single subject and single run.\n",
        "\n",
        "  Args:\n",
        "    subject (str):      subject ID to load\n",
        "    experiment (str):   Name of experiment\n",
        "    run (int):          (0 or 1)\n",
        "    remove_mean (bool): If True, subtract the parcel-wise mean (typically the mean BOLD signal is not of interest)\n",
        "    # WHY?\n",
        "\n",
        "  Returns\n",
        "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
        "\n",
        "  \"\"\"\n",
        "  bold_run  = RUNS[run]\n",
        "  bold_path = f\"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}\"\n",
        "  bold_file = \"data.npy\"\n",
        "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
        "  if remove_mean:\n",
        "    ts -= ts.mean(axis=1, keepdims=True)\n",
        "  return ts\n",
        "\n",
        "\n",
        "# print(EXPERIMENTS)\n",
        "# start computes start time in terms of frames, divides onset times by repitition time TR, round to integer\n",
        "# duration computes length of trial in terms of frames by dividing duration time by TR, round up to integer\n",
        "# frames = for each trial, generate range of frames corresponding to trial duration and start time..\n",
        "def load_evs(subject, experiment, run):\n",
        "  \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
        "\n",
        "  Args:\n",
        "    subject (str): subject ID to load\n",
        "    experiment (str) : Name of experiment\n",
        "    run (int): 0 or 1\n",
        "\n",
        "  Returns\n",
        "    evs (list of lists): A list of frames associated with each condition\n",
        "\n",
        "  \"\"\"\n",
        "  frames_list = []\n",
        "  task_key = f'tfMRI_{experiment}_{RUNS[run]}'\n",
        "  for cond in EXPERIMENTS[experiment]['cond']:\n",
        "    ev_file  = f\"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt\"\n",
        "    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
        "    ev       = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
        "    # Determine when trial starts, rounded down\n",
        "    start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
        "    # Use trial duration to determine how many frames to include for trial\n",
        "    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
        "    # Take the range of frames that correspond to this specific trial\n",
        "    print(start)\n",
        "    print(start.shape)\n",
        "    print(duration)\n",
        "    print(duration.shape)\n",
        "    print(list(zip(start,duration)))\n",
        "    frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
        "    print(frames)\n",
        "    frames_list.append(frames)\n",
        "\n",
        "  return frames_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05PHYsqLQRwC"
      },
      "source": [
        "## Let's try with emotion..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r6_IigzQQPh",
        "outputId": "48905b95-7e9e-4e61-afa5-2b8b27776e6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(360, 176)\n"
          ]
        }
      ],
      "source": [
        "my_exp = 'EMOTION'\n",
        "my_subj = subjects[1]\n",
        "my_run = 1\n",
        "\n",
        "data = load_single_timeseries(subject=my_subj,\n",
        "                              experiment=my_exp,\n",
        "                              run=my_run,\n",
        "                              remove_mean=True)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RbSWhHBcQWTQ",
        "outputId": "ebe5539f-3b13-4c41-a29d-6d7f3fb66cec"
      },
      "outputs": [],
      "source": [
        "evs = load_evs(subject=my_subj, experiment=my_exp, run=my_run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "V5U-ZRWVRRWP",
        "outputId": "dbe41cf9-3660-4809-b1f5-c48472d7efc0"
      },
      "outputs": [],
      "source": [
        "# A function to average all frames from any given condition\n",
        "\n",
        "def average_frames3(data, evs, experiment, cond):\n",
        "    # Find the index of the given condition within the experiment\n",
        "    idx = EXPERIMENTS[experiment]['cond'].index(cond)\n",
        "\n",
        "    # List to store the mean data for each set of event frames\n",
        "    mean_data_list = []\n",
        "\n",
        "    # Iterate over each set of event frames for the given condition\n",
        "    for i in range(len(evs[idx])):\n",
        "        # Debugging print statements\n",
        "        print(f\"\\nProcessing event frame set {i + 1}/{len(evs[idx])} for condition '{cond}'\")\n",
        "        print(f\"Event indices: {evs[idx][i]}\")\n",
        "        print(f\"Data shape before extraction: {data.shape}\")\n",
        "\n",
        "        try:\n",
        "            # Extract the data corresponding to the current set of event frames\n",
        "            current_data = data[:, evs[idx][i]]\n",
        "            print(f\"Extracted data shape: {current_data.shape}\")\n",
        "        except IndexError as e:\n",
        "            print(f\"IndexError: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Compute the mean of the extracted data along the time axis\n",
        "        mean_current_data = np.mean(current_data, axis=1, keepdims=True)\n",
        "        print(f\"Mean current data shape: {mean_current_data.shape}\")\n",
        "\n",
        "        # Append the mean data to the list\n",
        "        mean_data_list.append(mean_current_data)\n",
        "\n",
        "    if not mean_data_list:\n",
        "        print(\"Error: No valid event frames found\")\n",
        "        return None\n",
        "\n",
        "    stacked_means = np.stack(mean_data_list, axis=-1)\n",
        "    print(f\"Stacked means shape: {stacked_means.shape}\")\n",
        "\n",
        "    #return overall_mean\n",
        "    return stacked_means\n",
        "\n",
        "fr_activity = average_frames3(data, evs, my_exp, 'fear')\n",
        "nt_activity = average_frames3(data, evs, my_exp, 'neut')\n",
        "#contrast = fr_activity - nt_activity  # difference between 'fear' and 'neutral' conditions\n",
        "if fr_activity is not None and nt_activity is not None:\n",
        "    final_concatenated_means = np.concatenate((fr_activity, nt_activity), axis=-1)\n",
        "    print(f\"Final concatenated means shape: {final_concatenated_means.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LMlfJhI-C0A8",
        "outputId": "f974f997-861b-46dc-dc5e-961555f7e022"
      },
      "outputs": [],
      "source": [
        "all_trials = []\n",
        "all_labels = []\n",
        "\n",
        "for i in range(len(subjects)):\n",
        "    for r in [0, 1]:\n",
        "        data = load_single_timeseries(subject=subjects[i],\n",
        "                                      experiment=my_exp,\n",
        "                                      run=r,\n",
        "                                      remove_mean=True)\n",
        "\n",
        "        # Get the trials for both conditions\n",
        "        neut_trials = average_frames3(data, evs, my_exp, 'neut')\n",
        "        fear_trials = average_frames3(data, evs, my_exp, 'fear')\n",
        "\n",
        "        # Append each trial individually, flattened\n",
        "        if neut_trials is not None:\n",
        "            for trial in neut_trials.transpose(2, 0, 1):  # Iterate over trials and reshape\n",
        "                all_trials.append(trial.flatten())\n",
        "                all_labels.append(0)\n",
        "        if fear_trials is not None:\n",
        "            for trial in fear_trials.transpose(2, 0, 1):  # Iterate over trials and reshape\n",
        "                all_trials.append(trial.flatten())\n",
        "                all_labels.append(1)\n",
        "\n",
        "# Convert to numpy array\n",
        "all_trials_array = np.array(all_trials)\n",
        "\n",
        "# Write trials and labels to separate csv files\n",
        "np.savetxt('all_trials.csv', all_trials_array, delimiter=',')\n",
        "np.savetxt('all_labels.csv', all_labels, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A96g4-TJbvn"
      },
      "source": [
        "#Begin Deep Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy2PylUFJlYX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import f1_score as f1\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import *\n",
        "\n",
        "#-- Pytorch specific libraries import -----#\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7Dp35LtASBO"
      },
      "source": [
        "#Make it more complicated!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg1QKlmpAWAE"
      },
      "outputs": [],
      "source": [
        "# write a foor loop o iterate over 360 input features (brain regions)\n",
        "df_trials = pd.read_csv(\"./all_trials.csv\")\n",
        "df_labels = pd.read_csv(\"./all_labels.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWhJlT_cBjvg",
        "outputId": "bbe3fc0a-1ea6-4bb3-9f95-bbe782e424cb"
      },
      "outputs": [],
      "source": [
        "#Train & Test Set\n",
        "X= df_trials.iloc[: , :-1]\n",
        "y= df_labels.iloc[: , -1] #target at the end column\n",
        "\n",
        "train_x,test_x,train_y,test_y = train_test_split(X,y,random_state=42,test_size=0.2)\n",
        "print(\"\\n--Training data samples--\")\n",
        "print(train_x.shape)\n",
        "print(test_x.shape)\n",
        "print(\"\\n--Testing data samples--\")\n",
        "print(train_y.shape)\n",
        "print(test_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wTQBiczuB_4u"
      },
      "outputs": [],
      "source": [
        "###First use a MinMaxscaler to scale all the features of Train & Test dataframes\n",
        "\n",
        "scaler = preprocessing.MinMaxScaler() #normalizes the features\n",
        "x_train = scaler.fit_transform(train_x.values)\n",
        "x_test =  scaler.fit_transform(test_x.values)\n",
        "\n",
        "#print(\"Scaled values of Train set \\n\")\n",
        "#print(x_train)\n",
        "#print(\"\\nScaled values of Test set \\n\")\n",
        "#print(x_test)\n",
        "\n",
        "\n",
        "###Then convert the Train and Test sets into Tensors\n",
        "\n",
        "x_tensor =  torch.from_numpy(x_train).float() #converts numpy array to pytorch tensor\n",
        "y_tensor =  torch.from_numpy(train_y.values.ravel()).float() #flattens the array into a one-dimensional array\n",
        "xtest_tensor =  torch.from_numpy(x_test).float()\n",
        "ytest_tensor =  torch.from_numpy(test_y.values.ravel()).float() #flattens the array into a one-dimensional array\n",
        "\n",
        "#print(\"\\nTrain set Tensors \\n\")\n",
        "#print(x_tensor)\n",
        "#print(y_tensor)\n",
        "#print(\"\\nTest set Tensors \\n\")\n",
        "#print(xtest_tensor)\n",
        "#print(ytest_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ukMOOYllTl8",
        "outputId": "54303dce-894f-4f62-ecc2-2f5885b3602e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po-UNtbZB_zK"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ChurnModel(nn.Module):\n",
        "    def __init__(self, n_input_dim):\n",
        "        super(ChurnModel, self).__init__()\n",
        "        self.n_hidden1 = 720\n",
        "        self.n_hidden2 = 720\n",
        "        self.n_output = 1\n",
        "        self.layer_1 = nn.Linear(n_input_dim, self.n_hidden1)\n",
        "        self.layer_2 = nn.Linear(self.n_hidden1, self.n_hidden2)\n",
        "        self.layer_out = nn.Linear(self.n_hidden2, self.n_output)\n",
        "\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid =  nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(self.n_hidden1)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(self.n_hidden2)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.sigmoid(self.layer_out(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "#model = ChurnModel(n_input_dim)\n",
        "#print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-ydHjdr2B_mF"
      },
      "outputs": [],
      "source": [
        "# This loss function is typically used for binary classification tasks where the model outputs probabilities (e.g., values between 0 and 1)\n",
        "loss_func = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L0O-BfRB-aK"
      },
      "outputs": [],
      "source": [
        "y_true_test = test_y.values.ravel() #This method flattens the array into a one-dimensional format.\n",
        "conf_matrix = confusion_matrix(y_true_test ,ytest_pred)\n",
        "print(\"Confusion Matrix of the Test Set\")\n",
        "print(\"-----------\")\n",
        "print(conf_matrix)\n",
        "#Precision is defined as the ratio of true positive predictions to the total predicted positives\n",
        "print(\"Precision of the MLP :\\t\"+str(precision_score(y_true_test,ytest_pred)))\n",
        "print(\"Recall of the MLP    :\\t\"+str(recall_score(y_true_test,ytest_pred)))\n",
        "print(\"F1 Score of the Model :\\t\"+str(f1_score(y_true_test,ytest_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS5fxeHQuTSP"
      },
      "source": [
        "#Random Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "A5nFoPDhC1Ht",
        "outputId": "bb8d9769-4efb-4913-f3c5-16ab382253cc"
      },
      "outputs": [],
      "source": [
        "#Define a batch size ,\n",
        "bs = 32\n",
        "x_tensor =  torch.from_numpy(x_train).float() #converts numpy array to pytorch tensor\n",
        "y_tensor =  torch.from_numpy(train_y.values.ravel()).float() #flattens the array into a one-dimensional array\n",
        "xtest_tensor =  torch.from_numpy(x_test).float()\n",
        "ytest_tensor =  torch.from_numpy(test_y.values.ravel()).float() #flattens the array into a one-dimensional array\n",
        "y_tensor = y_tensor.unsqueeze(1)\n",
        "\n",
        "#For the validation/test dataset\n",
        "ytest_tensor = ytest_tensor.unsqueeze(1)\n",
        "test_ds = TensorDataset(xtest_tensor, ytest_tensor)\n",
        "test_loader = DataLoader(test_ds, batch_size=32)#model will process 32 samples at a time\n",
        "accuracies = []\n",
        "#num_ROIs = x_tensor.shape[1]\n",
        "new_x_tensor = x_tensor#[:, indices][:, second_indices][:, third_indices][:, fourth_indices][:, fifth_indices][:, sixth_indices][:, seventh_indices][:, eigth_indices]\n",
        "print(new_x_tensor)\n",
        "num_ROIs = new_x_tensor.shape[1]\n",
        "new_xtest_tensor = xtest_tensor#[:, indices][:, second_indices][:, third_indices][:, fourth_indices][:, fifth_indices][:, sixth_indices][:, seventh_indices][:, eigth_indices]\n",
        "print(num_ROIs)\n",
        "epochs = 50\n",
        "\n",
        "\n",
        "random_best_accs = []\n",
        "\n",
        "#200 randome samples\n",
        "for i in range(200):\n",
        "  # choose random indices from the 359 original brain regions with the length of 6\n",
        "  random_indices = np.random.choice(num_ROIs, size=6, replace=False)\n",
        "  # first seven brain regions (i.e. ~random)\n",
        "  # x_subset = new_x_tensor[:, :7]\n",
        "  # last seven brain regions (i.e. ~random)\n",
        "  # x_subset = new_x_tensor[:, -7:]\n",
        "  x_subset = new_x_tensor[:, random_indices]\n",
        "  # create a tensor dataset with the new subset of x\n",
        "  train_ds = TensorDataset(x_subset, y_tensor)\n",
        "  train_dl = DataLoader(train_ds, batch_size=bs)\n",
        "  test_ds = TensorDataset(new_xtest_tensor[:, random_indices], ytest_tensor)\n",
        "  test_loader = DataLoader(test_ds, batch_size=32)\n",
        "  n_input_dim = x_subset.shape[1]\n",
        "  model = ChurnModel(n_input_dim)\n",
        "  # 0.001 lr for everything before fifth_indices\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "  train_loss = []\n",
        "  val_acc = []\n",
        "  for epoch in range(epochs):\n",
        "      model.train()\n",
        "      # Within each epoch run the subsets of data = batch sizes\n",
        "      epoch_loss = 0\n",
        "      for xb, yb in train_dl:\n",
        "          y_pred = model(xb)            # Forward Propagation\n",
        "          loss = loss_func(y_pred, yb)  # Loss Computation\n",
        "          epoch_loss += loss.item()\n",
        "          optimizer.zero_grad()         # Clearing all previous gradients, setting to zero\n",
        "          loss.backward()               # Back Propagation\n",
        "          optimizer.step()              # Updating the parameters\n",
        "      #print(f\"Loss @ Epoch #{epoch}: {epoch_loss:.4f}\")\n",
        "      train_loss.append(epoch_loss)\n",
        "      # get validation accuracy for each epoch\n",
        "      v_acc = evaluate_model(model, test_loader)\n",
        "      val_acc.append(v_acc)\n",
        "\n",
        "  # we are getting the maximum test accuracy from all 50 training epochs for each model\n",
        "  accuracy = max(val_acc)\n",
        "  random_best_accs.append(accuracy)\n",
        "  save_file = r'/content/drive/My Drive/random_6_mlp_acc.npy'\n",
        "  np.save(save_file, random_best_accs)\n",
        "\n",
        "  #print(f'{len(accuracies)} models trained! c:')\n",
        "  print(\"ACCURACY\", accuracy)\n",
        "  plt.figure()\n",
        "  plt.plot(train_loss, label='Training Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.title(f'Model #{i}')\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(val_acc, label='Validation Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.title(f'Model #{i}')\n",
        "\n",
        "  if i % 5 == 0 and i != 0:\n",
        "    plt.figure()\n",
        "    plt.hist(random_best_accs)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "guOnk1HdQXhR",
        "outputId": "f69e5321-f5c2-4b72-8d18-385a71fb1cbd"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.hist(random_best_accs)\n",
        "plt.title('Random Sampling for 6 Brain Regions')\n",
        "plt.xlabel('Accuracy')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl6qC8ccj6ft"
      },
      "source": [
        "#Automate the process of getting new indices from the subset of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNrJu8ghDY14"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "        for xb, yb in test_loader:\n",
        "            y_pred = model(xb)  # Get model predictions\n",
        "            y_pred_tag = torch.round(y_pred)  # Round predictions to get binary outputs\n",
        "            correct += (y_pred_tag.eq(yb).sum().item())  # Count correct predictions\n",
        "            total += yb.size(0)  # Count total samples\n",
        "\n",
        "    accuracy = correct / total  # Calculate accuracy\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4qBtpJBtO81"
      },
      "source": [
        "#Final Sampling - Automated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kq7oj_P1Zm6I",
        "outputId": "60217f3c-3ff0-4675-f173-ef40bb6d52ec"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "\n",
        "def get_indices_from_file (file_path, cutoff):\n",
        "  data = np.load(file_path)\n",
        "  indices_bool = (data >= cutoff)\n",
        "  indices_bool = np.invert(indices_bool)\n",
        "  indices = np.where(indices_bool)[0]\n",
        "  return indices\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#target_num_ROIs = int(2/3*num_ROIs)\n",
        "num_ROIs = x_tensor.shape[1]\n",
        "\n",
        "def find_optimal_cutoffs(file_path):\n",
        "  accuracy_data = np.load(file_path)\n",
        "  cutoff = 1\n",
        "  step = 0.005\n",
        "  num_ROIs = len(accuracy_data)\n",
        "  target_num_ROIs = int(2/3*num_ROIs)\n",
        "\n",
        "  while num_ROIs > target_num_ROIs:\n",
        "    cutoff -= step\n",
        "    num_ROIs = len(get_indices_from_file(file_path, cutoff))\n",
        "\n",
        "  #check if previous cutoff was closer to the target\n",
        "  prev_num_ROIs = len(get_indices_from_file(file_path, cutoff+step))\n",
        "  if abs(prev_num_ROIs - target_num_ROIs) < abs(num_ROIs - target_num_ROIs):\n",
        "    cutoff += step\n",
        "\n",
        "  return cutoff\n",
        "\n",
        "bs = 32\n",
        "x_tensor =  torch.from_numpy(x_train).float() #converts numpy array to pytorch tensor\n",
        "y_tensor =  torch.from_numpy(train_y.values.ravel()).float() #flattens the array into a one-dimensional array\n",
        "xtest_tensor =  torch.from_numpy(x_test).float()\n",
        "ytest_tensor =  torch.from_numpy(test_y.values.ravel()).float() #flattens the array into a one-dimensional array\n",
        "y_tensor = y_tensor.unsqueeze(1)\n",
        "\n",
        "#For the validation/test dataset\n",
        "ytest_tensor = ytest_tensor.unsqueeze(1)\n",
        "test_ds = TensorDataset(xtest_tensor, ytest_tensor)\n",
        "test_loader = DataLoader(test_ds, batch_size=32)#model will process 32 samples at a time\n",
        "accuracies = []\n",
        "\n",
        "#TODO remove-this is just for debugging\n",
        "#x_tensor = x_tensor[:, :6]; num_rounds = 2\n",
        "\n",
        "num_rounds = 9 #eventually i wanna do it for 8 rounds just like the original code\n",
        "for k in range(num_rounds):\n",
        "  # explicity make sure we have a copy of the tensor\n",
        "  new_x_tensor = x_tensor.detach().clone()#.to('cuda')\n",
        "  new_xtest_tensor = xtest_tensor.detach().clone()#.to('cuda')\n",
        "  ROI_names = np.array(all_ROI_names.copy())\n",
        "  # we want to do this unless it's the first time:\n",
        "  # load the ith accuracy file, get its indices via cutoff function, then apply\n",
        "  for l in range(k): # this for loop is for trim trim trim!!!\n",
        "    file_path = f'/content/drive/My Drive/{l+1}_mlp_acc.npy'\n",
        "    cutoff = find_optimal_cutoffs(file_path)\n",
        "    indices = get_indices_from_file(file_path, cutoff)\n",
        "    new_x_tensor = new_x_tensor[:, indices]\n",
        "    new_xtest_tensor = new_xtest_tensor[:, indices]\n",
        "    ROI_names = ROI_names[indices]\n",
        "    print(ROI_names)\n",
        "\n",
        "  num_ROIs = new_x_tensor.shape[1]\n",
        "  accuracies = []\n",
        "  for i in range(num_ROIs):\n",
        "    ROI_indices = [j for j in range(num_ROIs) if j!=i]\n",
        "    #select a subset of brain regions with highest accuracies\n",
        "    x_subset = new_x_tensor[:, ROI_indices]\n",
        "    #x_subset = x_tensor[:,ROI_indices]\n",
        "    #create a tensor dataset with the new subset of x\n",
        "    #train_ds = TensorDataset(x_subset.to('cuda'), y_tensor.to('cuda'))\n",
        "    train_ds = TensorDataset(x_subset, y_tensor)\n",
        "    train_dl = DataLoader(train_ds, batch_size=bs)\n",
        "    #test_ds = TensorDataset(new_xtest_tensor[:,ROI_indices], ytest_tensor.to('cuda'))\n",
        "    test_ds = TensorDataset(new_xtest_tensor[:,ROI_indices], ytest_tensor)\n",
        "    test_loader = DataLoader(test_ds, batch_size=32)\n",
        "    n_input_dim = x_subset.shape[1]\n",
        "    model = ChurnModel(n_input_dim)\n",
        "    #model = model.to('cuda')\n",
        "    # 0.001 for everything before fifth_indices\n",
        "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
        "\n",
        "    train_loss = []\n",
        "    val_acc = []\n",
        "    epochs = 50\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        #Within each epoch run the subsets of data = batch sizes.\n",
        "        epoch_loss = 0\n",
        "        for xb, yb in train_dl:\n",
        "            #print(xb.shape)\n",
        "            y_pred = model(xb)            # Forward Propagation\n",
        "            #print(y_pred)\n",
        "            #print(yb)\n",
        "            loss = loss_func(y_pred, yb)  # Loss Computation\n",
        "            epoch_loss += loss.item()\n",
        "            optimizer.zero_grad()         # Clearing all previous gradients, setting to zero\n",
        "            loss.backward()               # Back Propagation\n",
        "            optimizer.step()              # Updating the parameters\n",
        "        #print(f\"Loss @ Epoch #{epoch}: {epoch_loss:.4f}\")\n",
        "        train_loss.append(epoch_loss)\n",
        "        # get validation accuracy\n",
        "        v_acc = evaluate_model(model, test_loader)\n",
        "        val_acc.append(v_acc)\n",
        "\n",
        "    # we are getting the maximum test accuracy from all 50 training epochs for each model\n",
        "    accuracy = max(val_acc)\n",
        "    accuracies.append(accuracy)\n",
        "    save_file = f'/content/drive/My Drive/{k+1}_mlp_acc.npy'\n",
        "    np.save(save_file, accuracies)\n",
        "    print(f'{len(accuracies)} models trained! c:')\n",
        "    print(\"ACCURACY\", accuracy)\n",
        "    plt.figure()\n",
        "    plt.plot(train_loss, label='Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title(f'Model #{i}')\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title(f'Model #{i}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2B0wrBCo65S"
      },
      "outputs": [],
      "source": [
        "for i in range(num_rounds): # this for loop is for trim trim trim!!!\n",
        "    file_path = f'/content/drive/My Drive/{i+1}_mlp_acc.npy'\n",
        "    accuracy_data = np.load(file_path)\n",
        "    plt.figure()\n",
        "    plt.hist(accuracy_data, label=\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.title(f'Trim Round #{i+1} Results')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni6L9jbyFhOQ"
      },
      "outputs": [],
      "source": [
        "# automate learning rate tuning\n",
        "\n",
        "# train the n 'test' (e.g. 3) networks/models\n",
        "\n",
        "# look at the validation accuracies over epochs / training\n",
        "\n",
        "# make sure each network has at least k (e.g. 3) new best validation accuracies\n",
        "# ('peaks') during the first j (e.g. 10) epochs\n",
        "\n",
        "# if not, decrease learning rate by 2x (i.e. multiple by 0.5)\n",
        "# and re-run the n 'test' networks.\n",
        "\n",
        "# once these criteria are met, we can t rain all the networks, not just the n 'test' ones."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
